{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import feature_extraction, linear_model, model_selection, preprocessing\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_df = pd.read_csv(r'E:\\Python\\Datasets\\Disaster_Tweets\\train.csv')\n",
    "twitter_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Data columns (total 12 columns):\n",
      " #   Column                 Non-Null Count  Dtype \n",
      "---  ------                 --------------  ----- \n",
      " 0   id                     7613 non-null   int64 \n",
      " 1   keyword                7552 non-null   object\n",
      " 2   location               5080 non-null   object\n",
      " 3   text                   7613 non-null   object\n",
      " 4   target                 7613 non-null   int64 \n",
      " 5   cleaned_text           7613 non-null   object\n",
      " 6   tokenized              7613 non-null   object\n",
      " 7   stopwords_removed      7613 non-null   object\n",
      " 8   porter_stemmer         7613 non-null   object\n",
      " 9   lemmatize_word         7613 non-null   object\n",
      " 10  porter_stemmer_joined  7613 non-null   object\n",
      " 11  lemmatize_word_joined  7613 non-null   object\n",
      "dtypes: int64(2), object(10)\n",
      "memory usage: 713.8+ KB\n"
     ]
    }
   ],
   "source": [
    "twitter_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4342\n",
       "1    3271\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>our deeds are the reason of this #earthquake m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask. canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>all residents asked to 'shelter in place' are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>just got sent this photo from ruby #alaska as ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                       cleaned_text  \n",
       "0       1  our deeds are the reason of this #earthquake m...  \n",
       "1       1             forest fire near la ronge sask. canada  \n",
       "2       1  all residents asked to 'shelter in place' are ...  \n",
       "3       1  13,000 people receive #wildfires evacuation or...  \n",
       "4       1  just got sent this photo from ruby #alaska as ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lower case\n",
    "\n",
    "twitter_df['cleaned_text'] = twitter_df['text'].apply(lambda x: x.lower())\n",
    "twitter_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Expand the Contrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand the Contrations\n",
    "import contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I would like to know how I would done that!'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "contractions.fix(\"I'd like to know how I'd done that!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expanding the Contrations\n",
    "twitter_df['cleaned_text'] = twitter_df['cleaned_text'].apply(contractions.fix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'I can't have kids cuz I got in a bicycle accident &amp; split my testicles. it's impossible for me to have kids' MICHAEL YOU ARE THE FATHER\n",
      "'i can not have kids cuz i got in a bicycle accident &amp; split my testicles. it is impossible for me to have kids' michael you are the father\n"
     ]
    }
   ],
   "source": [
    "# Checking contractions\n",
    "print(twitter_df[\"text\"][67])\n",
    "print(twitter_df[\"cleaned_text\"][67])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Noise Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1 Remove URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_URL(text):\n",
    "    \"\"\"\n",
    "        Remove URLs from a simple string\n",
    "    \"\"\"\n",
    "    return re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove url from the text\n",
    "twitter_df['cleaned_text'] = twitter_df['cleaned_text'].apply(remove_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@bbcmtd Wholesale Markets ablaze http://t.co/lHYXEOHY6C\n",
      "@bbcmtd wholesale markets ablaze \n"
     ]
    }
   ],
   "source": [
    "# Checking\n",
    "print(twitter_df[\"text\"][31])\n",
    "print(twitter_df[\"cleaned_text\"][31])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2 Remove HTML tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_tag(text):\n",
    "    \"\"\"\n",
    "        Remove html tags from a simple text\n",
    "    \"\"\"\n",
    "    html_tag = re.compile(r\"<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});\")\n",
    "    return re.sub(html_tag, \"\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove html tags from text\n",
    "twitter_df['cleaned_text'] = twitter_df['cleaned_text'].apply(remove_html_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rene Ablaze &amp; Jacinta - Secret 2k13 (Fallen Skies Edit) - Mar 30 2013  https://t.co/7MLMsUzV1Z\n",
      "rene ablaze  jacinta - secret 2k13 (fallen skies edit) - mar 30 2013  \n"
     ]
    }
   ],
   "source": [
    "# Checking\n",
    "print(twitter_df[\"text\"][62])\n",
    "print(twitter_df[\"cleaned_text\"][62])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3 Remove Non-ASCI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removal_non_ascii(text):\n",
    "    return re.sub(r'[^\\x00-\\x7f]', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df['cleaned_text'] = twitter_df['cleaned_text'].apply(removal_non_ascii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Barbados #Bridgetown JAMAICA ÛÒ Two cars set ablaze: SANTA CRUZ ÛÓ Head of the St Elizabeth Police Superintende...  http://t.co/wDUEaj8Q4J\n",
      "barbados #bridgetown jamaica  two cars set ablaze: santa cruz  head of the st elizabeth police superintende...  \n"
     ]
    }
   ],
   "source": [
    "# Checking\n",
    "print(twitter_df[\"text\"][38])\n",
    "print(twitter_df[\"cleaned_text\"][38])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Remove punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def remove_punc(text):\n",
    "#     return re.sub(r'[]!\"$%&\\'()*+,./:;=#@?[\\\\^_`{|}~-]+', \"\", text)\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df['cleaned_text'] = twitter_df['cleaned_text'].apply(remove_punc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#RockyFire Update => California Hwy. 20 closed in both directions due to Lake County fire - #CAfire #wildfires\n",
      "rockyfire update  california hwy 20 closed in both directions due to lake county fire  cafire wildfires\n"
     ]
    }
   ],
   "source": [
    "# Checking\n",
    "print(twitter_df[\"text\"][5])\n",
    "print(twitter_df[\"cleaned_text\"][5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Tokeinzation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>our deeds are the reason of this earthquake ma...</td>\n",
       "      <td>[our, deeds, are, the, reason, of, this, earth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>all residents asked to shelter in place are be...</td>\n",
       "      <td>[all, residents, asked, to, shelter, in, place...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>13000 people receive wildfires evacuation orde...</td>\n",
       "      <td>[13000, people, receive, wildfires, evacuation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>just got sent this photo from ruby alaska as s...</td>\n",
       "      <td>[just, got, sent, this, photo, from, ruby, ala...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                       cleaned_text  \\\n",
       "0       1  our deeds are the reason of this earthquake ma...   \n",
       "1       1              forest fire near la ronge sask canada   \n",
       "2       1  all residents asked to shelter in place are be...   \n",
       "3       1  13000 people receive wildfires evacuation orde...   \n",
       "4       1  just got sent this photo from ruby alaska as s...   \n",
       "\n",
       "                                           tokenized  \n",
       "0  [our, deeds, are, the, reason, of, this, earth...  \n",
       "1      [forest, fire, near, la, ronge, sask, canada]  \n",
       "2  [all, residents, asked, to, shelter, in, place...  \n",
       "3  [13000, people, receive, wildfires, evacuation...  \n",
       "4  [just, got, sent, this, photo, from, ruby, ala...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "twitter_df['tokenized'] = twitter_df['cleaned_text'].apply(word_tokenize)\n",
    "twitter_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.Removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\maksb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>stopwords_removed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>our deeds are the reason of this earthquake ma...</td>\n",
       "      <td>[our, deeds, are, the, reason, of, this, earth...</td>\n",
       "      <td>[deeds, reason, earthquake, may, allah, forgiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
       "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>all residents asked to shelter in place are be...</td>\n",
       "      <td>[all, residents, asked, to, shelter, in, place...</td>\n",
       "      <td>[residents, asked, shelter, place, notified, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>13000 people receive wildfires evacuation orde...</td>\n",
       "      <td>[13000, people, receive, wildfires, evacuation...</td>\n",
       "      <td>[13000, people, receive, wildfires, evacuation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>just got sent this photo from ruby alaska as s...</td>\n",
       "      <td>[just, got, sent, this, photo, from, ruby, ala...</td>\n",
       "      <td>[got, sent, photo, ruby, alaska, smoke, wildfi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                       cleaned_text  \\\n",
       "0       1  our deeds are the reason of this earthquake ma...   \n",
       "1       1              forest fire near la ronge sask canada   \n",
       "2       1  all residents asked to shelter in place are be...   \n",
       "3       1  13000 people receive wildfires evacuation orde...   \n",
       "4       1  just got sent this photo from ruby alaska as s...   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  [our, deeds, are, the, reason, of, this, earth...   \n",
       "1      [forest, fire, near, la, ronge, sask, canada]   \n",
       "2  [all, residents, asked, to, shelter, in, place...   \n",
       "3  [13000, people, receive, wildfires, evacuation...   \n",
       "4  [just, got, sent, this, photo, from, ruby, ala...   \n",
       "\n",
       "                                   stopwords_removed  \n",
       "0  [deeds, reason, earthquake, may, allah, forgiv...  \n",
       "1      [forest, fire, near, la, ronge, sask, canada]  \n",
       "2  [residents, asked, shelter, place, notified, o...  \n",
       "3  [13000, people, receive, wildfires, evacuation...  \n",
       "4  [got, sent, photo, ruby, alaska, smoke, wildfi...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "twitter_df['stopwords_removed'] = twitter_df['tokenized'].apply(lambda x: [word for word in x if word not in stop])\n",
    "twitter_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def porter_stemmer(text):\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    stems = [stemmer.stem(word) for word in text]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>stopwords_removed</th>\n",
       "      <th>porter_stemmer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>our deeds are the reason of this earthquake ma...</td>\n",
       "      <td>[our, deeds, are, the, reason, of, this, earth...</td>\n",
       "      <td>[deeds, reason, earthquake, may, allah, forgiv...</td>\n",
       "      <td>[deed, reason, earthquak, may, allah, forgiv, us]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
       "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
       "      <td>[forest, fire, near, la, rong, sask, canada]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>all residents asked to shelter in place are be...</td>\n",
       "      <td>[all, residents, asked, to, shelter, in, place...</td>\n",
       "      <td>[residents, asked, shelter, place, notified, o...</td>\n",
       "      <td>[resid, ask, shelter, place, notifi, offic, ev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>13000 people receive wildfires evacuation orde...</td>\n",
       "      <td>[13000, people, receive, wildfires, evacuation...</td>\n",
       "      <td>[13000, people, receive, wildfires, evacuation...</td>\n",
       "      <td>[13000, peopl, receiv, wildfir, evacu, order, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>just got sent this photo from ruby alaska as s...</td>\n",
       "      <td>[just, got, sent, this, photo, from, ruby, ala...</td>\n",
       "      <td>[got, sent, photo, ruby, alaska, smoke, wildfi...</td>\n",
       "      <td>[got, sent, photo, rubi, alaska, smoke, wildfi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                       cleaned_text  \\\n",
       "0       1  our deeds are the reason of this earthquake ma...   \n",
       "1       1              forest fire near la ronge sask canada   \n",
       "2       1  all residents asked to shelter in place are be...   \n",
       "3       1  13000 people receive wildfires evacuation orde...   \n",
       "4       1  just got sent this photo from ruby alaska as s...   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  [our, deeds, are, the, reason, of, this, earth...   \n",
       "1      [forest, fire, near, la, ronge, sask, canada]   \n",
       "2  [all, residents, asked, to, shelter, in, place...   \n",
       "3  [13000, people, receive, wildfires, evacuation...   \n",
       "4  [just, got, sent, this, photo, from, ruby, ala...   \n",
       "\n",
       "                                   stopwords_removed  \\\n",
       "0  [deeds, reason, earthquake, may, allah, forgiv...   \n",
       "1      [forest, fire, near, la, ronge, sask, canada]   \n",
       "2  [residents, asked, shelter, place, notified, o...   \n",
       "3  [13000, people, receive, wildfires, evacuation...   \n",
       "4  [got, sent, photo, ruby, alaska, smoke, wildfi...   \n",
       "\n",
       "                                      porter_stemmer  \n",
       "0  [deed, reason, earthquak, may, allah, forgiv, us]  \n",
       "1       [forest, fire, near, la, rong, sask, canada]  \n",
       "2  [resid, ask, shelter, place, notifi, offic, ev...  \n",
       "3  [13000, peopl, receiv, wildfir, evacu, order, ...  \n",
       "4  [got, sent, photo, rubi, alaska, smoke, wildfi...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_df['porter_stemmer'] = twitter_df['stopwords_removed'].apply(porter_stemmer)\n",
    "twitter_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def lemmatize_word(text):\n",
    "    \"\"\"\n",
    "        Lemmatize the tokenized words\n",
    "    \"\"\"\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemma = [lemmatizer.lemmatize(word, tag) for word, tag in text]\n",
    "    return lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>stopwords_removed</th>\n",
       "      <th>porter_stemmer</th>\n",
       "      <th>lemmatize_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>our deeds are the reason of this earthquake ma...</td>\n",
       "      <td>[our, deeds, are, the, reason, of, this, earth...</td>\n",
       "      <td>[deeds, reason, earthquake, may, allah, forgiv...</td>\n",
       "      <td>[deed, reason, earthquak, may, allah, forgiv, us]</td>\n",
       "      <td>[deed, reason, earthquake, may, allah, forgive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
       "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
       "      <td>[forest, fire, near, la, rong, sask, canada]</td>\n",
       "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>all residents asked to shelter in place are be...</td>\n",
       "      <td>[all, residents, asked, to, shelter, in, place...</td>\n",
       "      <td>[residents, asked, shelter, place, notified, o...</td>\n",
       "      <td>[resid, ask, shelter, place, notifi, offic, ev...</td>\n",
       "      <td>[resident, asked, shelter, place, notified, of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>13000 people receive wildfires evacuation orde...</td>\n",
       "      <td>[13000, people, receive, wildfires, evacuation...</td>\n",
       "      <td>[13000, people, receive, wildfires, evacuation...</td>\n",
       "      <td>[13000, peopl, receiv, wildfir, evacu, order, ...</td>\n",
       "      <td>[13000, people, receive, wildfire, evacuation,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>just got sent this photo from ruby alaska as s...</td>\n",
       "      <td>[just, got, sent, this, photo, from, ruby, ala...</td>\n",
       "      <td>[got, sent, photo, ruby, alaska, smoke, wildfi...</td>\n",
       "      <td>[got, sent, photo, rubi, alaska, smoke, wildfi...</td>\n",
       "      <td>[got, sent, photo, ruby, alaska, smoke, wildfi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                       cleaned_text  \\\n",
       "0       1  our deeds are the reason of this earthquake ma...   \n",
       "1       1              forest fire near la ronge sask canada   \n",
       "2       1  all residents asked to shelter in place are be...   \n",
       "3       1  13000 people receive wildfires evacuation orde...   \n",
       "4       1  just got sent this photo from ruby alaska as s...   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  [our, deeds, are, the, reason, of, this, earth...   \n",
       "1      [forest, fire, near, la, ronge, sask, canada]   \n",
       "2  [all, residents, asked, to, shelter, in, place...   \n",
       "3  [13000, people, receive, wildfires, evacuation...   \n",
       "4  [just, got, sent, this, photo, from, ruby, ala...   \n",
       "\n",
       "                                   stopwords_removed  \\\n",
       "0  [deeds, reason, earthquake, may, allah, forgiv...   \n",
       "1      [forest, fire, near, la, ronge, sask, canada]   \n",
       "2  [residents, asked, shelter, place, notified, o...   \n",
       "3  [13000, people, receive, wildfires, evacuation...   \n",
       "4  [got, sent, photo, ruby, alaska, smoke, wildfi...   \n",
       "\n",
       "                                      porter_stemmer  \\\n",
       "0  [deed, reason, earthquak, may, allah, forgiv, us]   \n",
       "1       [forest, fire, near, la, rong, sask, canada]   \n",
       "2  [resid, ask, shelter, place, notifi, offic, ev...   \n",
       "3  [13000, peopl, receiv, wildfir, evacu, order, ...   \n",
       "4  [got, sent, photo, rubi, alaska, smoke, wildfi...   \n",
       "\n",
       "                                      lemmatize_word  \n",
       "0  [deed, reason, earthquake, may, allah, forgive...  \n",
       "1      [forest, fire, near, la, ronge, sask, canada]  \n",
       "2  [resident, asked, shelter, place, notified, of...  \n",
       "3  [13000, people, receive, wildfire, evacuation,...  \n",
       "4  [got, sent, photo, ruby, alaska, smoke, wildfi...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "twitter_df['lemmatize_word'] = twitter_df['stopwords_removed'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "twitter_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df['porter_stemmer_joined'] = twitter_df['porter_stemmer'].apply(lambda x: ' '.join(x))\n",
    "twitter_df['lemmatize_word_joined'] = twitter_df['lemmatize_word'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_preporcessing_methods_with_prediction(X, y, alpha = list(np.linspace(1,50,10))):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, random_state = 1)\n",
    "    pipeline = Pipeline([('CountVectorizer', feature_extraction.text.CountVectorizer()), ('classifier', linear_model.RidgeClassifier())])\n",
    "    parametrs = {'classifier__alpha': alpha,\n",
    "             'classifier__class_weight': [None, 'balanced'],\n",
    "             'classifier__copy_X': [True],\n",
    "             'classifier__fit_intercept': [True],\n",
    "             'classifier__max_iter': [None],\n",
    "             'classifier__normalize': [False],\n",
    "             'classifier__random_state': [0],\n",
    "             'classifier__solver': ['auto'],\n",
    "             'classifier__tol': [0.001]}\n",
    "    clf_grid = GridSearchCV(pipeline, param_grid=parametrs, cv = 3)\n",
    "    clf_grid.fit(X_train, y_train)\n",
    "    print('Best Score:', clf_grid.best_score_)\n",
    "    print('Best Params', clf_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.7961114030478192\n",
      "Best Params {'classifier__alpha': 14.071428571428573, 'classifier__class_weight': None, 'classifier__copy_X': True, 'classifier__fit_intercept': True, 'classifier__max_iter': None, 'classifier__normalize': False, 'classifier__random_state': 0, 'classifier__solver': 'auto', 'classifier__tol': 0.001}\n"
     ]
    }
   ],
   "source": [
    "check_preporcessing_methods_with_prediction(twitter_df['porter_stemmer_joined'],twitter_df['target'], list(np.linspace(7,16,15)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.7999649675950254\n",
      "Best Params {'classifier__alpha': 27.428571428571427, 'classifier__class_weight': 'balanced', 'classifier__copy_X': True, 'classifier__fit_intercept': True, 'classifier__max_iter': None, 'classifier__normalize': False, 'classifier__random_state': 0, 'classifier__solver': 'auto', 'classifier__tol': 0.001}\n"
     ]
    }
   ],
   "source": [
    "check_preporcessing_methods_with_prediction(twitter_df['lemmatize_word_joined'],twitter_df['target'] ,list(np.linspace(24,28,15)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(twitter_df['text'], twitter_df['target'], random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = feature_extraction.text.CountVectorizer()\n",
    "train_vectors = count_vectorizer.fit_transform(X_train)\n",
    "test_vectors = count_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_model_clf = linear_model.RidgeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores = model_selection.cross_val_score(ridge_model_clf, train_vectors, y_train, cv = 3, scoring='f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.70415335, 0.72426938, 0.7343245 ])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,\n",
       "                max_iter=None, normalize=False, random_state=None,\n",
       "                solver='auto', tol=0.001)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_model_clf.fit(train_vectors, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = ridge_model_clf.predict(test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7390728476821192"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(prediction, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_test = pd.read_csv(r'E:\\Python\\Datasets\\Disaster_Tweets\\test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 1.0,\n",
       " 'class_weight': None,\n",
       " 'copy_X': True,\n",
       " 'fit_intercept': True,\n",
       " 'max_iter': None,\n",
       " 'normalize': False,\n",
       " 'random_state': None,\n",
       " 'solver': 'auto',\n",
       " 'tol': 0.001}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_model_clf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "pipeline = Pipeline([('CountVectorizer', feature_extraction.text.CountVectorizer()), ('classifier', linear_model.RidgeClassifier())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('CountVectorizer',\n",
       "                                        CountVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.int64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        preprocessor=None,\n",
       "                                                        stop_words=None,\n",
       "                                                        strip_accen...\n",
       "                                               44.55555555555556, 50.0],\n",
       "                         'classifier__class_weight': [None, 'balanced'],\n",
       "                         'classifier__copy_X': [True],\n",
       "                         'classifier__fit_intercept': [True],\n",
       "                         'classifier__max_iter': [None],\n",
       "                         'classifier__normalize': [False],\n",
       "                         'classifier__random_state': [0],\n",
       "                         'classifier__solver': ['auto'],\n",
       "                         'classifier__tol': [0.001]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parametrs = {'classifier__alpha': list(np.linspace(1,50,10)),\n",
    "             'classifier__class_weight': [None, 'balanced'],\n",
    "             'classifier__copy_X': [True],\n",
    "             'classifier__fit_intercept': [True],\n",
    "             'classifier__max_iter': [None],\n",
    "             'classifier__normalize': [False],\n",
    "             'classifier__random_state': [0],\n",
    "             'classifier__solver': ['auto'],\n",
    "             'classifier__tol': [0.001]}\n",
    "clf_grid = GridSearchCV(pipeline, param_grid=parametrs, cv = 3)\n",
    "clf_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8001401296198984"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier__alpha': 17.333333333333336,\n",
       " 'classifier__class_weight': None,\n",
       " 'classifier__copy_X': True,\n",
       " 'classifier__fit_intercept': True,\n",
       " 'classifier__max_iter': None,\n",
       " 'classifier__normalize': False,\n",
       " 'classifier__random_state': 0,\n",
       " 'classifier__solver': 'auto',\n",
       " 'classifier__tol': 0.001}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_best = linear_model.RidgeClassifier(alpha = 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RidgeClassifier(alpha=21, class_weight=None, copy_X=True, fit_intercept=True,\n",
       "                max_iter=None, normalize=False, random_state=None,\n",
       "                solver='auto', tol=0.001)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_best.fit(train_vectors, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.761049723756906"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(clf_best.predict(test_vectors),y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7451708565758928"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_scores = model_selection.cross_val_score(clf_best, train_vectors, y_train, cv = 3, scoring='f1')\n",
    "cv_scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier__alpha': 17.333333333333336,\n",
       " 'classifier__class_weight': None,\n",
       " 'classifier__copy_X': True,\n",
       " 'classifier__fit_intercept': True,\n",
       " 'classifier__max_iter': None,\n",
       " 'classifier__normalize': False,\n",
       " 'classifier__random_state': 0,\n",
       " 'classifier__solver': 'auto',\n",
       " 'classifier__tol': 0.001}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "             estimator=RidgeClassifier(alpha=1.0, class_weight=None,\n",
       "                                       copy_X=True, fit_intercept=True,\n",
       "                                       max_iter=None, normalize=False,\n",
       "                                       random_state=None, solver='auto',\n",
       "                                       tol=0.001),\n",
       "             iid='warn', n_jobs=None,\n",
       "             param_grid={'alpha': [0.1], 'class_weight': [None, 'balanced'],\n",
       "                         'copy_X': [True], 'fit_intercept': [True],\n",
       "                         'max_iter': [None], 'normalize': [False],\n",
       "                         'random_state': [0], 'solver': ['auto'],\n",
       "                         'tol': [0.001]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parametrs = {'alpha': [0.1],\n",
    "             'class_weight': [None, 'balanced'],\n",
    "             'copy_X': [True],\n",
    "             'fit_intercept': [True],\n",
    "             'max_iter': [None],\n",
    "             'normalize': [False],\n",
    "             'random_state': [0],\n",
    "             'solver': ['auto'],\n",
    "             'tol': [0.001]}\n",
    "clf_grid = GridSearchCV(ridge_model_clf, param_grid=parametrs, cv = 3)\n",
    "clf_grid.fit(train_vectors, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7565247854265196"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def TFIDF(data, ngram = 1):\n",
    "    tfidf_x = TfidfVectorizer(ngram_range = (ngram, ngram))\n",
    "    emb = tfidf_x.fit_transform(data).toarray()\n",
    "    return emb, tfidf_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_preporcessing_methods_with_prediction(X, y, alpha = list(np.linspace(1,50,10))):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, random_state = 1)\n",
    "    pipeline = Pipeline([('TfidfVectorizer', TfidfVectorizer()), ('classifier', linear_model.RidgeClassifier())])\n",
    "    parametrs = {'classifier__alpha': alpha,\n",
    "             'classifier__class_weight': [None, 'balanced'],\n",
    "             'classifier__copy_X': [True],\n",
    "             'classifier__fit_intercept': [True],\n",
    "             'classifier__max_iter': [None],\n",
    "             'classifier__normalize': [False],\n",
    "             'classifier__random_state': [0],\n",
    "             'classifier__solver': ['auto'],\n",
    "             'classifier__tol': [0.001],\n",
    "             'TfidfVectorizer__ngram_range': [(1,1), (1,2), (1,3)]}\n",
    "    clf_grid = GridSearchCV(pipeline, param_grid=parametrs, cv = 3, scoring= 'f1')\n",
    "    clf_grid.fit(X, y)\n",
    "    print('Best Score:', clf_grid.best_score_)\n",
    "    print('Best Params', clf_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.6684860934652413\n",
      "Best Params {'TfidfVectorizer__ngram_range': (1, 3), 'classifier__alpha': 1.5214285714285714, 'classifier__class_weight': 'balanced', 'classifier__copy_X': True, 'classifier__fit_intercept': True, 'classifier__max_iter': None, 'classifier__normalize': False, 'classifier__random_state': 0, 'classifier__solver': 'auto', 'classifier__tol': 0.001}\n"
     ]
    }
   ],
   "source": [
    "check_preporcessing_methods_with_prediction(twitter_df['porter_stemmer_joined'],twitter_df['target'], list(np.linspace(0.1,20,15)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.6671608529923245\n",
      "Best Params {'TfidfVectorizer__ngram_range': (1, 3), 'classifier__alpha': 2.9428571428571426, 'classifier__class_weight': 'balanced', 'classifier__copy_X': True, 'classifier__fit_intercept': True, 'classifier__max_iter': None, 'classifier__normalize': False, 'classifier__random_state': 0, 'classifier__solver': 'auto', 'classifier__tol': 0.001}\n"
     ]
    }
   ],
   "source": [
    "check_preporcessing_methods_with_prediction(twitter_df['lemmatize_word_joined'],twitter_df['target'], list(np.linspace(0.1,20,15)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Word2vec averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_path = ('E:/Python/Pretrained_vectors/GoogleNews-vectors-negative300.bin')\n",
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary = True, limit = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_vec(tokens_list, vector, generate_missing=False, k=300):\n",
    "    \"\"\"\n",
    "        Calculate average embedding value of sentence from each word vector\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(tokens_list)<1:\n",
    "        return np.zeros(k)\n",
    "    \n",
    "    if generate_missing:\n",
    "        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list]\n",
    "    else:\n",
    "        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]\n",
    "    \n",
    "    length = len(vectorized)\n",
    "    summed = np.sum(vectorized, axis=0)\n",
    "    averaged = np.divide(summed, length)\n",
    "    return averaged.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(vectors, text, generate_missing=False, k=300):\n",
    "    \"\"\"\n",
    "        create the sentence embedding\n",
    "    \"\"\"\n",
    "    embeddings = text.apply(lambda x: get_average_vec(x, vectors, generate_missing=generate_missing, k=k))\n",
    "    return list(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df['word2vec_averaged_vector'] = twitter_df['lemmatize_word'].apply(lambda x: get_average_vec(x,\\\n",
    "                                                    word2vec_model,generate_missing = True,k= 300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# twitter_df['word2vec_averaged_vector'] = twitter_df['word2vec_averaged_vector'].apply(lambda x: x.astype('float64'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_results(X, y, alpha = list(np.linspace(1,50,10))):\n",
    "    X = X.tolist()\n",
    "\n",
    "    parametrs = {'alpha': alpha,\n",
    "             'class_weight': [None, 'balanced'],\n",
    "             'copy_X': [True],\n",
    "             'fit_intercept': [True],\n",
    "             'max_iter': [None],\n",
    "             'normalize': [False],\n",
    "             'random_state': [0],\n",
    "             'solver': ['auto'],\n",
    "             'tol': [0.001]}\n",
    "    clf_grid = GridSearchCV(linear_model.RidgeClassifier(), param_grid=parametrs, cv = 3, scoring= 'f1', error_score='raise')\n",
    "    clf_grid.fit(X, y)\n",
    "    print('Best Score:', clf_grid.best_score_)\n",
    "    print('Best Params', clf_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.724441516783949\n",
      "Best Params {'alpha': 45.26315789473684, 'class_weight': 'balanced', 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'normalize': False, 'random_state': 0, 'solver': 'auto', 'tol': 0.001}\n"
     ]
    }
   ],
   "source": [
    "check_results(twitter_df['word2vec_averaged_vector'],twitter_df['target'], list(np.linspace(40,50,20)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge =linear_model.RidgeClassifier(alpha = 1)\n",
    "train_vectors = twitter_df['word2vec_averaged_vector'].tolist()\n",
    "y_train = twitter_df['target']\n",
    "cv_scores = model_selection.cross_val_score(ridge, train_vectors, y_train, cv = 4, scoring='f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.66441594, 0.68603214, 0.69149597, 0.74027604])"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores = model_selection.cross_val_score(ridge_model_clf, train_vectors, y_train, cv = 3, scoring='f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n",
      "float64\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "for el in twitter_df['word2vec_averaged_vector'][:3]:\n",
    "    print(el.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0872279554605484,\n",
       " 0.02663748525083065,\n",
       " 0.1637834757566452,\n",
       " 0.1142229363322258,\n",
       " -0.080078125,\n",
       " 0.0319998599588871,\n",
       " 0.1100027933716774,\n",
       " -0.0509033203125,\n",
       " 0.1780133992433548,\n",
       " 0.1765485554933548,\n",
       " -0.0840715691447258,\n",
       " -0.1289760023355484,\n",
       " -0.1637137234210968,\n",
       " 0.1197379007935524,\n",
       " -0.1378697007894516,\n",
       " 0.1565987765789032,\n",
       " 0.0750994011759758,\n",
       " 0.0575125552713871,\n",
       " -0.0322636179625988,\n",
       " -0.1509748250246048,\n",
       " 0.0736083984375,\n",
       " 0.0992780402302742,\n",
       " 0.2378627210855484,\n",
       " -0.0453578419983387,\n",
       " 0.05859375,\n",
       " -0.04541015625,\n",
       " -0.0623604916036129,\n",
       " -0.0647670179605484,\n",
       " -0.0679234117269516,\n",
       " -0.118408203125,\n",
       " -0.0903669074177742,\n",
       " -0.0027378627564758062,\n",
       " -0.1729736328125,\n",
       " 0.00250244140625,\n",
       " -0.0200020931661129,\n",
       " 0.09637995809316635,\n",
       " 0.0513044074177742,\n",
       " 0.01032366044819355,\n",
       " 0.0426112599670887,\n",
       " 0.12689208984375,\n",
       " 0.1547154039144516,\n",
       " 0.0467093326151371,\n",
       " 0.18359375,\n",
       " 0.02054269053041935,\n",
       " 0.1044049933552742,\n",
       " -0.15234375,\n",
       " -0.010027204640209675,\n",
       " -0.02840750478208065,\n",
       " -0.1530412882566452,\n",
       " -0.0673479363322258,\n",
       " -0.03759765625,\n",
       " 0.0440673828125,\n",
       " 0.0181906558573246,\n",
       " 0.0991777703166008,\n",
       " 0.0788399800658226,\n",
       " 0.1400495320558548,\n",
       " -0.1646902859210968,\n",
       " 0.0165546964854002,\n",
       " -0.0260489322245121,\n",
       " -0.1429269015789032,\n",
       " 0.0665261372923851,\n",
       " 0.0992780402302742,\n",
       " -0.07856369018554688,\n",
       " 0.0333644337952137,\n",
       " -0.0723005011677742,\n",
       " -0.1008126363158226,\n",
       " 0.0326625294983387,\n",
       " 0.01604352705180645,\n",
       " -0.0676792711019516,\n",
       " -0.0381556935608387,\n",
       " 0.0461077019572258,\n",
       " 0.02263532392680645,\n",
       " -0.058349609375,\n",
       " 0.0027095249388366938,\n",
       " -0.0838666632771492,\n",
       " -0.0327104851603508,\n",
       " 0.008649553172290325,\n",
       " 0.0567801333963871,\n",
       " 0.08602142333984375,\n",
       " -0.0796595960855484,\n",
       " -0.1031668558716774,\n",
       " 0.0330636166036129,\n",
       " -0.01422991044819355,\n",
       " 0.025634765625,\n",
       " -0.0636683851480484,\n",
       " 0.01639229990541935,\n",
       " -0.0560128353536129,\n",
       " 0.1255231648683548,\n",
       " 0.0840977281332016,\n",
       " -0.03106689453125,\n",
       " 0.00439453125,\n",
       " -0.0059988838620483875,\n",
       " -0.0019792828243225813,\n",
       " -0.0733468160033226,\n",
       " 0.0457589291036129,\n",
       " -0.0327322818338871,\n",
       " 0.0562918521463871,\n",
       " 0.0804617777466774,\n",
       " 0.1657889187335968,\n",
       " -0.049072265625,\n",
       " -0.2194475382566452,\n",
       " -0.020172119140625,\n",
       " 0.0723353773355484,\n",
       " 0.0778111070394516,\n",
       " 0.0624869205057621,\n",
       " -0.0477818064391613,\n",
       " -0.1105869859457016,\n",
       " -0.1185825914144516,\n",
       " 0.072265625,\n",
       " 0.1228201761841774,\n",
       " -0.1671491414308548,\n",
       " -0.1373814195394516,\n",
       " 0.01907784678041935,\n",
       " 0.0554460808634758,\n",
       " 0.0516183041036129,\n",
       " 0.01729910634458065,\n",
       " 0.04246193915605545,\n",
       " -0.0668247789144516,\n",
       " 0.0597970150411129,\n",
       " 0.1025390625,\n",
       " 0.0445382259786129,\n",
       " 0.1239166259765625,\n",
       " -0.1538434773683548,\n",
       " 0.02530343271791935,\n",
       " -0.0809587761759758,\n",
       " -0.005257742945104837,\n",
       " -0.1210414320230484,\n",
       " -0.1025041863322258,\n",
       " 0.1146676167845726,\n",
       " 0.01020159013569355,\n",
       " -0.1468505859375,\n",
       " -0.0162527896463871,\n",
       " -0.0420270636677742,\n",
       " -0.0448172427713871,\n",
       " 0.05908203125,\n",
       " -0.1025216206908226,\n",
       " -0.0358799509704113,\n",
       " -0.02037702314555645,\n",
       " 0.0947701558470726,\n",
       " 0.1151646226644516,\n",
       " 0.1611589640378952,\n",
       " -0.178802490234375,\n",
       " 0.0378592349588871,\n",
       " -0.0868617445230484,\n",
       " 0.0343279168009758,\n",
       " -0.0329938605427742,\n",
       " -0.0840279683470726,\n",
       " -0.0348249152302742,\n",
       " -0.1109444722533226,\n",
       " -0.1480189710855484,\n",
       " 0.0776192769408226,\n",
       " -0.0796072855591774,\n",
       " -0.0491681769490242,\n",
       " -0.0763811394572258,\n",
       " -0.0460030697286129,\n",
       " -0.0765468031167984,\n",
       " 0.0569893978536129,\n",
       " 0.1202130988240242,\n",
       " -0.02215140126645565,\n",
       " -0.004708426538854837,\n",
       " 0.0875941663980484,\n",
       " 0.02469308115541935,\n",
       " 0.0674525648355484,\n",
       " -0.0490025095641613,\n",
       " -0.08685302734375,\n",
       " -0.2089800089597702,\n",
       " 0.2269810289144516,\n",
       " -0.0042397635988891125,\n",
       " -0.1068987175822258,\n",
       " 0.01154436357319355,\n",
       " -0.010463169775903225,\n",
       " 0.070068359375,\n",
       " -0.1185389906167984,\n",
       " -0.0873325914144516,\n",
       " 0.006434849463403225,\n",
       " 0.013898576609790325,\n",
       " 0.1266697496175766,\n",
       " -0.0482875294983387,\n",
       " 0.0848475843667984,\n",
       " -0.1372419148683548,\n",
       " -0.07614489644765854,\n",
       " -0.01130022294819355,\n",
       " -0.0018986293580383062,\n",
       " -0.08781855553388596,\n",
       " 0.0780596062541008,\n",
       " 0.010846818797290325,\n",
       " -0.0890241339802742,\n",
       " 0.0368303582072258,\n",
       " 0.0799734964966774,\n",
       " 0.1023297980427742,\n",
       " -0.0365862175822258,\n",
       " 0.1260288804769516,\n",
       " -0.02141462080180645,\n",
       " 0.052001953125,\n",
       " -0.006103515625,\n",
       " 0.14067085087299347,\n",
       " 0.01581682451069355,\n",
       " 0.0418265201151371,\n",
       " -0.010266712866723537,\n",
       " -0.1515764445066452,\n",
       " -0.0698939710855484,\n",
       " -0.0567626953125,\n",
       " -0.1397705078125,\n",
       " -0.1557442843914032,\n",
       " -0.009190150536596775,\n",
       " -0.0663364976644516,\n",
       " -0.1009085550904274,\n",
       " 0.038330078125,\n",
       " 0.0897565558552742,\n",
       " 0.0630841925740242,\n",
       " -0.02069091796875,\n",
       " 0.0752476304769516,\n",
       " 0.0059640067629516125,\n",
       " -0.098876953125,\n",
       " -0.2640206515789032,\n",
       " 0.07644544541835785,\n",
       " 0.1031123548746109,\n",
       " 0.0167061947286129,\n",
       " -0.1201171875,\n",
       " -0.0052315848879516125,\n",
       " -0.02057756669819355,\n",
       " 0.004195076879113913,\n",
       " -0.0032261440064758062,\n",
       " -0.098388671875,\n",
       " 0.01834542490541935,\n",
       " -0.0779854878783226,\n",
       " 0.0468052439391613,\n",
       " 0.1322195827960968,\n",
       " -0.03671155497431755,\n",
       " -0.0225655697286129,\n",
       " 0.015276228077709675,\n",
       " 0.0440172478556633,\n",
       " -0.0763462632894516,\n",
       " 0.0247977115213871,\n",
       " 0.0811244398355484,\n",
       " 0.07177734375,\n",
       " 0.01079450361430645,\n",
       " -0.1326729953289032,\n",
       " 0.1260637491941452,\n",
       " 0.0611572265625,\n",
       " 0.1144670769572258,\n",
       " -0.0594744011759758,\n",
       " 0.0624171681702137,\n",
       " -0.12579345703125,\n",
       " 0.007638114038854837,\n",
       " 0.00899396650493145,\n",
       " 0.0330636166036129,\n",
       " 0.1235787495970726,\n",
       " -0.0546177439391613,\n",
       " -0.1194370836019516,\n",
       " 0.0403878353536129,\n",
       " 0.0643833726644516,\n",
       " 0.05035636946558952,\n",
       " 0.0371268130838871,\n",
       " 0.1398293673992157,\n",
       " -0.1046840101480484,\n",
       " 0.08681488037109375,\n",
       " 0.1264386922121048,\n",
       " 0.0647670179605484,\n",
       " -0.0622449591755867,\n",
       " -0.11322838813066483,\n",
       " -0.0683768168091774,\n",
       " -0.0499790720641613,\n",
       " 0.02596609853208065,\n",
       " -0.042724609375,\n",
       " 0.07776587456464767,\n",
       " -0.06884765625,\n",
       " 0.0186767578125,\n",
       " 0.0017438615905120969,\n",
       " -0.1469377726316452,\n",
       " -0.0709402933716774,\n",
       " 0.01968819834291935,\n",
       " 0.1500767320394516,\n",
       " 0.0560825876891613,\n",
       " -0.0011509486939758062,\n",
       " 0.0072195869870483875,\n",
       " -0.1317661851644516,\n",
       " -0.0218680240213871,\n",
       " -0.0704868882894516,\n",
       " 0.0831473246216774,\n",
       " 0.01094709150493145,\n",
       " 0.0167410708963871,\n",
       " -0.004813057836145163,\n",
       " 0.044921875,\n",
       " 0.02664620615541935,\n",
       " 0.0030735561158508062,\n",
       " 0.0466395802795887,\n",
       " 0.03072248212993145,\n",
       " -0.02690778486430645,\n",
       " 0.0778547003865242,\n",
       " -0.0580357126891613,\n",
       " 0.0831473246216774,\n",
       " -0.1333705335855484,\n",
       " -0.0794503316283226,\n",
       " -0.00390625,\n",
       " -0.01813398115336895,\n",
       " -0.12115478515625,\n",
       " -0.0953717902302742,\n",
       " -0.05255126953125,\n",
       " 0.14927509427070618]"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_df['word2vec_averaged_vector'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.08722796,  0.02663749,  0.16378348,  0.11422294, -0.08007812,\n",
       "        0.03199986,  0.11000279, -0.05090332,  0.1780134 ,  0.17654856,\n",
       "       -0.08407157, -0.128976  , -0.16371372,  0.1197379 , -0.1378697 ,\n",
       "        0.15659878,  0.0750994 ,  0.05751256, -0.03226362, -0.15097483,\n",
       "        0.0736084 ,  0.09927804,  0.23786272, -0.04535784,  0.05859375,\n",
       "       -0.04541016, -0.06236049, -0.06476702, -0.06792341, -0.1184082 ,\n",
       "       -0.09036691, -0.00273786, -0.17297363,  0.00250244, -0.02000209,\n",
       "        0.09637996,  0.05130441,  0.01032366,  0.04261126,  0.12689209,\n",
       "        0.1547154 ,  0.04670933,  0.18359375,  0.02054269,  0.10440499,\n",
       "       -0.15234375, -0.0100272 , -0.0284075 , -0.15304129, -0.06734794,\n",
       "       -0.03759766,  0.04406738,  0.01819066,  0.09917777,  0.07883998,\n",
       "        0.14004953, -0.16469029,  0.0165547 , -0.02604893, -0.1429269 ,\n",
       "        0.06652614,  0.09927804, -0.07856369,  0.03336443, -0.0723005 ,\n",
       "       -0.10081264,  0.03266253,  0.01604353, -0.06767927, -0.03815569,\n",
       "        0.0461077 ,  0.02263532, -0.05834961,  0.00270952, -0.08386666,\n",
       "       -0.03271049,  0.00864955,  0.05678013,  0.08602142, -0.0796596 ,\n",
       "       -0.10316686,  0.03306362, -0.01422991,  0.02563477, -0.06366839,\n",
       "        0.0163923 , -0.05601284,  0.12552316,  0.08409773, -0.03106689,\n",
       "        0.00439453, -0.00599888, -0.00197928, -0.07334682,  0.04575893,\n",
       "       -0.03273228,  0.05629185,  0.08046178,  0.16578892, -0.04907227,\n",
       "       -0.21944754, -0.02017212,  0.07233538,  0.07781111,  0.06248692,\n",
       "       -0.04778181, -0.11058699, -0.11858259,  0.07226562,  0.12282018,\n",
       "       -0.16714914, -0.13738142,  0.01907785,  0.05544608,  0.0516183 ,\n",
       "        0.01729911,  0.04246194, -0.06682478,  0.05979702,  0.10253906,\n",
       "        0.04453823,  0.12391663, -0.15384348,  0.02530343, -0.08095878,\n",
       "       -0.00525774, -0.12104143, -0.10250419,  0.11466762,  0.01020159,\n",
       "       -0.14685059, -0.01625279, -0.04202706, -0.04481724,  0.05908203,\n",
       "       -0.10252162, -0.03587995, -0.02037702,  0.09477016,  0.11516462,\n",
       "        0.16115896, -0.17880249,  0.03785923, -0.08686174,  0.03432792,\n",
       "       -0.03299386, -0.08402797, -0.03482492, -0.11094447, -0.14801897,\n",
       "        0.07761928, -0.07960729, -0.04916818, -0.07638114, -0.04600307,\n",
       "       -0.0765468 ,  0.0569894 ,  0.1202131 , -0.0221514 , -0.00470843,\n",
       "        0.08759417,  0.02469308,  0.06745256, -0.04900251, -0.08685303,\n",
       "       -0.20898001,  0.22698103, -0.00423976, -0.10689872,  0.01154436,\n",
       "       -0.01046317,  0.07006836, -0.11853899, -0.08733259,  0.00643485,\n",
       "        0.01389858,  0.12666975, -0.04828753,  0.08484758, -0.13724191,\n",
       "       -0.0761449 , -0.01130022, -0.00189863, -0.08781856,  0.07805961,\n",
       "        0.01084682, -0.08902413,  0.03683036,  0.0799735 ,  0.1023298 ,\n",
       "       -0.03658622,  0.12602888, -0.02141462,  0.05200195, -0.00610352,\n",
       "        0.14067085,  0.01581682,  0.04182652, -0.01026671, -0.15157644,\n",
       "       -0.06989397, -0.0567627 , -0.13977051, -0.15574428, -0.00919015,\n",
       "       -0.0663365 , -0.10090856,  0.03833008,  0.08975656,  0.06308419,\n",
       "       -0.02069092,  0.07524763,  0.00596401, -0.09887695, -0.26402065,\n",
       "        0.07644545,  0.10311235,  0.01670619, -0.12011719, -0.00523158,\n",
       "       -0.02057757,  0.00419508, -0.00322614, -0.09838867,  0.01834542,\n",
       "       -0.07798549,  0.04680524,  0.13221958, -0.03671155, -0.02256557,\n",
       "        0.01527623,  0.04401725, -0.07634626,  0.02479771,  0.08112444,\n",
       "        0.07177734,  0.0107945 , -0.132673  ,  0.12606375,  0.06115723,\n",
       "        0.11446708, -0.0594744 ,  0.06241717, -0.12579346,  0.00763811,\n",
       "        0.00899397,  0.03306362,  0.12357875, -0.05461774, -0.11943708,\n",
       "        0.04038784,  0.06438337,  0.05035637,  0.03712681,  0.13982937,\n",
       "       -0.10468401,  0.08681488,  0.12643869,  0.06476702, -0.06224496,\n",
       "       -0.11322839, -0.06837682, -0.04997907,  0.0259661 , -0.04272461,\n",
       "        0.07776587, -0.06884766,  0.01867676,  0.00174386, -0.14693777,\n",
       "       -0.07094029,  0.0196882 ,  0.15007673,  0.05608259, -0.00115095,\n",
       "        0.00721959, -0.13176619, -0.02186802, -0.07048689,  0.08314732,\n",
       "        0.01094709,  0.01674107, -0.00481306,  0.04492188,  0.02664621,\n",
       "        0.00307356,  0.04663958,  0.03072248, -0.02690778,  0.0778547 ,\n",
       "       -0.05803571,  0.08314732, -0.13337053, -0.07945033, -0.00390625,\n",
       "       -0.01813398, -0.12115479, -0.09537179, -0.05255127,  0.14927509])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Data columns (total 13 columns):\n",
      " #   Column                    Non-Null Count  Dtype \n",
      "---  ------                    --------------  ----- \n",
      " 0   id                        7613 non-null   int64 \n",
      " 1   keyword                   7552 non-null   object\n",
      " 2   location                  5080 non-null   object\n",
      " 3   text                      7613 non-null   object\n",
      " 4   target                    7613 non-null   int64 \n",
      " 5   cleaned_text              7613 non-null   object\n",
      " 6   tokenized                 7613 non-null   object\n",
      " 7   stopwords_removed         7613 non-null   object\n",
      " 8   porter_stemmer            7613 non-null   object\n",
      " 9   lemmatize_word            7613 non-null   object\n",
      " 10  porter_stemmer_joined     7613 non-null   object\n",
      " 11  lemmatize_word_joined     7613 non-null   object\n",
      " 12  word2vec_averaged_vector  7613 non-null   object\n",
      "dtypes: int64(2), object(11)\n",
      "memory usage: 773.3+ KB\n"
     ]
    }
   ],
   "source": [
    "twitter_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.08722796,  0.02663749,  0.16378348,  0.11422294, -0.08007812,\n",
       "        0.03199986,  0.11000279, -0.05090332,  0.1780134 ,  0.17654856,\n",
       "       -0.08407157, -0.128976  , -0.16371372,  0.1197379 , -0.1378697 ,\n",
       "        0.15659878,  0.0750994 ,  0.05751256, -0.03226362, -0.15097483,\n",
       "        0.0736084 ,  0.09927804,  0.23786272, -0.04535784,  0.05859375,\n",
       "       -0.04541016, -0.06236049, -0.06476702, -0.06792341, -0.1184082 ,\n",
       "       -0.09036691, -0.00273786, -0.17297363,  0.00250244, -0.02000209,\n",
       "        0.09637996,  0.05130441,  0.01032366,  0.04261126,  0.12689209,\n",
       "        0.1547154 ,  0.04670933,  0.18359375,  0.02054269,  0.10440499,\n",
       "       -0.15234375, -0.0100272 , -0.0284075 , -0.15304129, -0.06734794,\n",
       "       -0.03759766,  0.04406738,  0.01819066,  0.09917777,  0.07883998,\n",
       "        0.14004953, -0.16469029,  0.0165547 , -0.02604893, -0.1429269 ,\n",
       "        0.06652614,  0.09927804, -0.07856369,  0.03336443, -0.0723005 ,\n",
       "       -0.10081264,  0.03266253,  0.01604353, -0.06767927, -0.03815569,\n",
       "        0.0461077 ,  0.02263532, -0.05834961,  0.00270952, -0.08386666,\n",
       "       -0.03271049,  0.00864955,  0.05678013,  0.08602142, -0.0796596 ,\n",
       "       -0.10316686,  0.03306362, -0.01422991,  0.02563477, -0.06366839,\n",
       "        0.0163923 , -0.05601284,  0.12552316,  0.08409773, -0.03106689,\n",
       "        0.00439453, -0.00599888, -0.00197928, -0.07334682,  0.04575893,\n",
       "       -0.03273228,  0.05629185,  0.08046178,  0.16578892, -0.04907227,\n",
       "       -0.21944754, -0.02017212,  0.07233538,  0.07781111,  0.06248692,\n",
       "       -0.04778181, -0.11058699, -0.11858259,  0.07226562,  0.12282018,\n",
       "       -0.16714914, -0.13738142,  0.01907785,  0.05544608,  0.0516183 ,\n",
       "        0.01729911,  0.04246194, -0.06682478,  0.05979702,  0.10253906,\n",
       "        0.04453823,  0.12391663, -0.15384348,  0.02530343, -0.08095878,\n",
       "       -0.00525774, -0.12104143, -0.10250419,  0.11466762,  0.01020159,\n",
       "       -0.14685059, -0.01625279, -0.04202706, -0.04481724,  0.05908203,\n",
       "       -0.10252162, -0.03587995, -0.02037702,  0.09477016,  0.11516462,\n",
       "        0.16115896, -0.17880249,  0.03785923, -0.08686174,  0.03432792,\n",
       "       -0.03299386, -0.08402797, -0.03482492, -0.11094447, -0.14801897,\n",
       "        0.07761928, -0.07960729, -0.04916818, -0.07638114, -0.04600307,\n",
       "       -0.0765468 ,  0.0569894 ,  0.1202131 , -0.0221514 , -0.00470843,\n",
       "        0.08759417,  0.02469308,  0.06745256, -0.04900251, -0.08685303,\n",
       "       -0.20898001,  0.22698103, -0.00423976, -0.10689872,  0.01154436,\n",
       "       -0.01046317,  0.07006836, -0.11853899, -0.08733259,  0.00643485,\n",
       "        0.01389858,  0.12666975, -0.04828753,  0.08484758, -0.13724191,\n",
       "       -0.0761449 , -0.01130022, -0.00189863, -0.08781856,  0.07805961,\n",
       "        0.01084682, -0.08902413,  0.03683036,  0.0799735 ,  0.1023298 ,\n",
       "       -0.03658622,  0.12602888, -0.02141462,  0.05200195, -0.00610352,\n",
       "        0.14067085,  0.01581682,  0.04182652, -0.01026671, -0.15157644,\n",
       "       -0.06989397, -0.0567627 , -0.13977051, -0.15574428, -0.00919015,\n",
       "       -0.0663365 , -0.10090856,  0.03833008,  0.08975656,  0.06308419,\n",
       "       -0.02069092,  0.07524763,  0.00596401, -0.09887695, -0.26402065,\n",
       "        0.07644545,  0.10311235,  0.01670619, -0.12011719, -0.00523158,\n",
       "       -0.02057757,  0.00419508, -0.00322614, -0.09838867,  0.01834542,\n",
       "       -0.07798549,  0.04680524,  0.13221958, -0.03671155, -0.02256557,\n",
       "        0.01527623,  0.04401725, -0.07634626,  0.02479771,  0.08112444,\n",
       "        0.07177734,  0.0107945 , -0.132673  ,  0.12606375,  0.06115723,\n",
       "        0.11446708, -0.0594744 ,  0.06241717, -0.12579346,  0.00763811,\n",
       "        0.00899397,  0.03306362,  0.12357875, -0.05461774, -0.11943708,\n",
       "        0.04038784,  0.06438337,  0.05035637,  0.03712681,  0.13982937,\n",
       "       -0.10468401,  0.08681488,  0.12643869,  0.06476702, -0.06224496,\n",
       "       -0.11322839, -0.06837682, -0.04997907,  0.0259661 , -0.04272461,\n",
       "        0.07776587, -0.06884766,  0.01867676,  0.00174386, -0.14693777,\n",
       "       -0.07094029,  0.0196882 ,  0.15007673,  0.05608259, -0.00115095,\n",
       "        0.00721959, -0.13176619, -0.02186802, -0.07048689,  0.08314732,\n",
       "        0.01094709,  0.01674107, -0.00481306,  0.04492188,  0.02664621,\n",
       "        0.00307356,  0.04663958,  0.03072248, -0.02690778,  0.0778547 ,\n",
       "       -0.05803571,  0.08314732, -0.13337053, -0.07945033, -0.00390625,\n",
       "       -0.01813398, -0.12115479, -0.09537179, -0.05255127,  0.1492751 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making submission prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer_sumbission = feature_extraction.text.CountVectorizer()\n",
    "\n",
    "submission_train_vectors = count_vectorizer_sumbission.fit_transform(twitter_df['lemmatize_word_joined'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "TfidfVectorizer_sumbission = TfidfVectorizer(ngram_range = (1, 2))\n",
    "submission_train_vectors = TfidfVectorizer_sumbission.fit_transform(twitter_df['lemmatize_word_joined'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "twitter_test = pd.read_csv(r'E:\\Python\\Datasets\\Disaster_Tweets\\test.csv')\n",
    "\n",
    "twitter_test['cleaned_text'] = twitter_test['text'].apply(lambda x: x.lower())\n",
    "twitter_test['cleaned_text'] = twitter_test['cleaned_text'].apply(contractions.fix)\n",
    "twitter_test['cleaned_text'] = twitter_test['cleaned_text'].apply(remove_URL)\n",
    "twitter_test['cleaned_text'] = twitter_test['cleaned_text'].apply(remove_html_tag)\n",
    "twitter_test['cleaned_text'] = twitter_test['cleaned_text'].apply(removal_non_ascii)\n",
    "twitter_test['cleaned_text'] = twitter_test['cleaned_text'].apply(remove_punc)\n",
    "twitter_test['tokenized'] = twitter_test['cleaned_text'].apply(word_tokenize)\n",
    "twitter_test['stopwords_removed'] = twitter_test['tokenized'].apply(lambda x: [word for word in x if word not in stop])\n",
    "twitter_test['lemmatize_word'] = twitter_test['stopwords_removed'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "twitter_test['lemmatize_word_joined'] = twitter_test['lemmatize_word'].apply(lambda x: ' '.join(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "TfidfVectorizer - Vocabulary wasn't fitted.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-65-0f97196cabbc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msubmission_test_vectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfVectorizer_sumbission\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtwitter_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lemmatize_word_joined'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, raw_documents, copy)\u001b[0m\n\u001b[0;32m   1678\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_tfidf'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'The tfidf vector is not fitted'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1680\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1681\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1682\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   1107\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_vocabulary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1109\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_vocabulary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# use the same matrix-building strategy as fit_transform\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_check_vocabulary\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    387\u001b[0m         \u001b[1;34m\"\"\"Check if vocabulary is empty or missing (not fit-ed)\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"%(name)s - Vocabulary wasn't fitted.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 389\u001b[1;33m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'vocabulary_'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    390\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[1;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[0;32m    912\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    913\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mall_or_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mattributes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 914\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    915\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFittedError\u001b[0m: TfidfVectorizer - Vocabulary wasn't fitted."
     ]
    }
   ],
   "source": [
    "submission_test_vectors = count_vectorizer_sumbission.transform(twitter_test['lemmatize_word_joined'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_test_vectors = TfidfVectorizer_sumbission.transform(twitter_test['lemmatize_word_joined'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_best = linear_model.RidgeClassifier(alpha = 1.5)\n",
    "clf_best.fit(submission_train_vectors, twitter_df['target'])\n",
    "submission_prediction = clf_best.predict(submission_test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>stopwords_removed</th>\n",
       "      <th>lemmatize_word</th>\n",
       "      <th>lemmatize_word_joined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>just happened a terrible car crash</td>\n",
       "      <td>[just, happened, a, terrible, car, crash]</td>\n",
       "      <td>[happened, terrible, car, crash]</td>\n",
       "      <td>[happened, terrible, car, crash]</td>\n",
       "      <td>happened terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>heard about earthquake is different cities sta...</td>\n",
       "      <td>[heard, about, earthquake, is, different, citi...</td>\n",
       "      <td>[heard, earthquake, different, cities, stay, s...</td>\n",
       "      <td>[heard, earthquake, different, city, stay, saf...</td>\n",
       "      <td>heard earthquake different city stay safe ever...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>there is a forest fire at spot pond geese are ...</td>\n",
       "      <td>[there, is, a, forest, fire, at, spot, pond, g...</td>\n",
       "      <td>[forest, fire, spot, pond, geese, fleeing, acr...</td>\n",
       "      <td>[forest, fire, spot, pond, goose, fleeing, acr...</td>\n",
       "      <td>forest fire spot pond goose fleeing across str...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "      <td>apocalypse lighting spokane wildfires</td>\n",
       "      <td>[apocalypse, lighting, spokane, wildfires]</td>\n",
       "      <td>[apocalypse, lighting, spokane, wildfires]</td>\n",
       "      <td>[apocalypse, lighting, spokane, wildfire]</td>\n",
       "      <td>apocalypse lighting spokane wildfire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "      <td>typhoon soudelor kills 28 in china and taiwan</td>\n",
       "      <td>[typhoon, soudelor, kills, 28, in, china, and,...</td>\n",
       "      <td>[typhoon, soudelor, kills, 28, china, taiwan]</td>\n",
       "      <td>[typhoon, soudelor, kill, 28, china, taiwan]</td>\n",
       "      <td>typhoon soudelor kill 28 china taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash   \n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...   \n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...   \n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires   \n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0                 just happened a terrible car crash   \n",
       "1  heard about earthquake is different cities sta...   \n",
       "2  there is a forest fire at spot pond geese are ...   \n",
       "3              apocalypse lighting spokane wildfires   \n",
       "4      typhoon soudelor kills 28 in china and taiwan   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0          [just, happened, a, terrible, car, crash]   \n",
       "1  [heard, about, earthquake, is, different, citi...   \n",
       "2  [there, is, a, forest, fire, at, spot, pond, g...   \n",
       "3         [apocalypse, lighting, spokane, wildfires]   \n",
       "4  [typhoon, soudelor, kills, 28, in, china, and,...   \n",
       "\n",
       "                                   stopwords_removed  \\\n",
       "0                   [happened, terrible, car, crash]   \n",
       "1  [heard, earthquake, different, cities, stay, s...   \n",
       "2  [forest, fire, spot, pond, geese, fleeing, acr...   \n",
       "3         [apocalypse, lighting, spokane, wildfires]   \n",
       "4      [typhoon, soudelor, kills, 28, china, taiwan]   \n",
       "\n",
       "                                      lemmatize_word  \\\n",
       "0                   [happened, terrible, car, crash]   \n",
       "1  [heard, earthquake, different, city, stay, saf...   \n",
       "2  [forest, fire, spot, pond, goose, fleeing, acr...   \n",
       "3          [apocalypse, lighting, spokane, wildfire]   \n",
       "4       [typhoon, soudelor, kill, 28, china, taiwan]   \n",
       "\n",
       "                               lemmatize_word_joined  \n",
       "0                        happened terrible car crash  \n",
       "1  heard earthquake different city stay safe ever...  \n",
       "2  forest fire spot pond goose fleeing across str...  \n",
       "3               apocalypse lighting spokane wildfire  \n",
       "4              typhoon soudelor kill 28 china taiwan  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = twitter_test[['id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id\n",
       "0   0\n",
       "1   2\n",
       "2   3\n",
       "3   9\n",
       "4  11"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = twitter_test[['id']].copy()\n",
    "submission_df['target'] = submission_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target\n",
       "0   0       1\n",
       "1   2       1\n",
       "2   3       1\n",
       "3   9       1\n",
       "4  11       1"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv(r'E:\\Python\\Datasets\\Disaster_Tweets\\submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making submission Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_test = pd.read_csv(r'E:\\Python\\Datasets\\Disaster_Tweets\\test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_test['cleaned_text'] = twitter_test['text'].apply(lambda x: x.lower())\n",
    "twitter_test['cleaned_text'] = twitter_test['cleaned_text'].apply(contractions.fix)\n",
    "twitter_test['cleaned_text'] = twitter_test['cleaned_text'].apply(remove_URL)\n",
    "twitter_test['cleaned_text'] = twitter_test['cleaned_text'].apply(remove_html_tag)\n",
    "twitter_test['cleaned_text'] = twitter_test['cleaned_text'].apply(removal_non_ascii)\n",
    "twitter_test['cleaned_text'] = twitter_test['cleaned_text'].apply(remove_punc)\n",
    "twitter_test['tokenized'] = twitter_test['cleaned_text'].apply(word_tokenize)\n",
    "twitter_test['stopwords_removed'] = twitter_test['tokenized'].apply(lambda x: [word for word in x if word not in stop])\n",
    "twitter_test['lemmatize_word'] = twitter_test['stopwords_removed'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "twitter_test['word2vec_average'] = twitter_test['lemmatize_word'].apply(lambda x: get_average_vec(x, word2vec_model, generate_missing=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_clf = linear_model.RidgeClassifier(alpha = 45)\n",
    "ridge_clf.fit(twitter_df['word2vec_averaged_vector'].tolist(), twitter_df['target'])\n",
    "prediction = ridge_clf.predict(twitter_test['word2vec_average'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = twitter_test[['id']].copy()\n",
    "submission_df['target'] = prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target\n",
       "0   0       1\n",
       "1   2       0\n",
       "2   3       1\n",
       "3   9       1\n",
       "4  11       1"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv(r'E:\\Python\\Datasets\\Disaster_Tweets\\submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
